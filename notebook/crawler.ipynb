{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第一題 #\n",
    "讀取2017所有表特版的文章"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import datetime\n",
    "from urllib.parse import urlparse\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# function part #\n",
    "\n",
    "* check_date_from_page\n",
    "傳進頁面的soup，檢查是否存在12/31號的文章\n",
    "若有，回傳該文章之soup\n",
    "\n",
    "* get_website\n",
    "傳入url，回傳該網站的網站位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_date_from_page(cur_article):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    def check_date(soup):\n",
    "        \"\"\"\n",
    "        input beautifuy object, check date\n",
    "        must find last\n",
    "        \"\"\"\n",
    "        #get date\n",
    "        date_string=soup.find('div', 'date').string.strip()\n",
    "        date = datetime.datetime.strptime(date_string,\"%m/%d\")\n",
    "        if date.month!=12 or date.day!=31:\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "        \n",
    "    cur_article = cur_article.find('div', 'r-ent')\n",
    "    while cur_article is not None:\n",
    "        if not check_date(cur_article):\n",
    "            # next\n",
    "            cur_article = cur_article.find_next('div', 'r-ent')\n",
    "        else:\n",
    "            # find last match\n",
    "            while cur_article is not None:\n",
    "                prev_article = cur_article\n",
    "                cur_article = cur_article.find_next('div', 'r-ent')\n",
    "                # got 1/1\n",
    "                if not check_date(cur_article):\n",
    "                    return prev_article\n",
    "            return prev_article\n",
    "    return False\n",
    "    \n",
    "def get_website(url):\n",
    "    \"\"\"\n",
    "    get the website from url \n",
    "    \"\"\"\n",
    "    site = urlparse(url)\n",
    "    site_netloc = site.scheme + '://' + site.netloc\n",
    "    return site_netloc\n",
    "\n",
    "def get_response(url):\n",
    "    response = requests.get(url)\n",
    "    while not response.ok:\n",
    "        response = requests.get(url)\n",
    "    return response\n",
    "\n",
    "def get_prev_page_url(root_soup):\n",
    "    return  website + root_soup.find('div', id='action-bar-container').find('div', 'btn-group-paging').find_all('a')[1]['href']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 沒網址的不用爬，可忽略\n",
    "* 爬2017年的所有文章\n",
    "* 忽略分類為公告的文章\n",
    "* output:\n",
    "* 1. 所有文章 \n",
    "  2. 所有爆文\n",
    "* 檔案內容 - 日期,標題,url（無空格\n",
    "\n",
    "## 結束條件 ##\n",
    "https://www.ptt.cc/bbs/Beauty/M.1483208018.A.411.html\n",
    "\n",
    "# main function start #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page: https://www.ptt.cc/bbs/Beauty/index.html not find!\n",
      "get next page...\n",
      "first page, direct jump ...\n",
      "jump to:  https://www.ptt.cc/bbs/Beauty/index2352.html\n",
      "page: https://www.ptt.cc/bbs/Beauty/index2352.html not find!\n",
      "get next page...\n",
      "page: https://www.ptt.cc/bbs/Beauty/index2351.html not find!\n",
      "get next page...\n",
      "page: https://www.ptt.cc/bbs/Beauty/index2350.html not find!\n",
      "get next page...\n",
      "page: https://www.ptt.cc/bbs/Beauty/index2349.html not find!\n",
      "get next page...\n",
      "page: https://www.ptt.cc/bbs/Beauty/index2348.html not find!\n",
      "get next page...\n",
      "page: https://www.ptt.cc/bbs/Beauty/index2347.html not find!\n",
      "get next page...\n",
      "page: https://www.ptt.cc/bbs/Beauty/index2346.html not find!\n",
      "get next page...\n",
      "page: https://www.ptt.cc/bbs/Beauty/index2345.html not find!\n",
      "get next page...\n",
      "page: https://www.ptt.cc/bbs/Beauty/index2344.html not find!\n",
      "get next page...\n",
      "page: https://www.ptt.cc/bbs/Beauty/index2343.html not find!\n",
      "get next page...\n",
      "page: https://www.ptt.cc/bbs/Beauty/index2342.html not find!\n",
      "get next page...\n",
      "page: https://www.ptt.cc/bbs/Beauty/index2341.html not find!\n",
      "get next page...\n",
      "got ya\n"
     ]
    }
   ],
   "source": [
    "# find the 12/31 article\n",
    "# END IN this post\n",
    "END_POST = '[正妹] 瘦身日本妹'\n",
    "url = \"https://www.ptt.cc/bbs/Beauty/index.html\"\n",
    "# get ptt's website\n",
    "website = get_website(url)\n",
    "# first flag\n",
    "first_flag = True\n",
    "############### loading ####################\n",
    "response = get_response(url)\n",
    "root_soup = BeautifulSoup(response.text, 'lxml')\n",
    "############### end loading ################\n",
    "find_soup = check_date_from_page(root_soup)\n",
    "\n",
    "while not find_soup:\n",
    "    # find next page\n",
    "    print('page: ' + url + ' not find!')\n",
    "    print('get next page...')\n",
    "    # prev page's url\n",
    "    url = get_prev_page_url(root_soup)\n",
    "    \n",
    "    # if this is prist page, than direct minus 300 ...\n",
    "    if first_flag == True:\n",
    "        print(\"first page, direct jump ...\")\n",
    "        # got the number part\n",
    "        match = re.search(r'(\\d+)(?:.html$)', url)\n",
    "        number = match[1]\n",
    "        number = int(number) - 300\n",
    "        # replace back\n",
    "        url = re.sub(r\"(\\d+)(.html$)\", str(number) + r'\\2', url)\n",
    "        print('jump to: ', url)\n",
    "        first_flag = False\n",
    "    \n",
    "    # loading\n",
    "    response = get_response(url)\n",
    "    root_soup = BeautifulSoup(response.text, 'lxml')\n",
    "    find_soup = check_date_from_page(root_soup)\n",
    "    \n",
    "print('got ya')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# function for next part of code #\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date(soup):\n",
    "    date_string = soup.find('div', 'date').string.strip()\n",
    "    return date_string\n",
    "\n",
    "def get_title(cur_soup):\n",
    "    if cur_soup.find('div','title').find('a'):\n",
    "        title = ''\n",
    "        for string in cur_soup.find('div', 'title').a.strings:\n",
    "            #if @ occur, need specical parse\n",
    "            if string == '[email protected]':\n",
    "                title = title '@' + string['data-cfemail']\n",
    "            else:\n",
    "                title = title + string\n",
    "\n",
    "        title = cur_soup.find('div','title').a.string.strip()\n",
    "        return title\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def write_info(all_file, pop_file, soup):\n",
    "    \"\"\"soup is on <div r-ent>\"\"\"\n",
    "    def check_title_string(title):\n",
    "        \"\"\"title is a string, true mean this is what we need\"\"\"\n",
    "        match = re.search(r'\\[公告\\]', title)\n",
    "        if not match:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    # check if title.a exist\n",
    "    if not soup.find('div', 'title').find('a'):\n",
    "        return False\n",
    "    #check title contain [公告] or not\n",
    "    title = get_title(soup)\n",
    "    if check_title_string(title):\n",
    "        # grab date, topic, url\n",
    "        date_temp = soup.find('div', 'date').string.strip()\n",
    "        # remove /\n",
    "        date = re.sub(r'[/]', '', date_temp)\n",
    "        # title\n",
    "        url = website + soup.find('div', 'title').find('a')['href']\n",
    "        # merge to one line\n",
    "        write_line = ','.join([date, title, url])\n",
    "        # if 爆, write to pop_file\n",
    "        thumb_number = soup.find('div', 'nrec')\n",
    "        if thumb_number.find('span'):\n",
    "            thumb_number = thumb_number.span.string.strip()\n",
    "        else:\n",
    "            tuhmb_number = ''\n",
    "        if thumb_number == '爆':\n",
    "            pop_file.write(write_line+'\\n')\n",
    "        all_file.write(write_line+'\\n')\n",
    "        # write to all_file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "write file for \"[正妹] 2018 世界新車大展 Day 1\"\n",
      "write file for \"[正妹] 2018\"\n",
      "write file for \"[正妹] 奶特\"\n",
      "write file for \"[正妹] 韓流\"\n",
      "write file for \"[正妹] 彩彩小萌獸\"\n",
      "write file for \"[正妹] 柏木由紀\"\n",
      "write file for \"[正妹] 俄羅斯暗黑女模Helga Lovekaty\"\n",
      "write file for \"[正妹] 彩美旬果\"\n",
      "write file for \"[正妹] 秀氣長髮\"\n",
      "write file for \"[神人] 台北車展Polo beats DJ\"\n",
      "write file for \"Re: [神人] hotels.com廣告女主角\"\n",
      "write file for \"[正妹] 台北車展lexus正妹\"\n",
      "write file for \"[正妹] 韓流女團藝人\"\n",
      "write file for \"[神人] 自問自答\"\n",
      "write file for \"[神人]台北車展land rover的sg\"\n",
      "write file for \"[正妹] 跨年前發一波\"\n",
      "write file for \"[帥哥] 最近好像流行這種\"\n",
      "write file for \"[正妹] 企業排球聯賽 中纖美女主攻手\"\n",
      "write file for \"[正妹] 無題\"\n",
      "write file for \"[正妹] 世界第衣美\"\n",
      "write file for \"[正妹] 高畑充希 Takahata Mitsuki\"\n",
      "write file for \"[正妹] 兩個高三生\"\n",
      "write file for \"[神人] 柯批剪綵旁的正妹\"\n",
      "write file for \"[正妹] Madison Iseman\"\n",
      "write file for \"[正妹] 李元玲\"\n",
      "write file for \"[帥哥] 正太控請進\"\n",
      "write file for \"[正妹] 浜辺美波\"\n",
      "write file for \"[正妹] IU\"\n",
      "write file for \"[正妹] 阿旦\"\n",
      "write file for \"[正妹] 小松菜奈2017\"\n",
      "write file for \"[正妹] 收到來信 想試試水溫的鄉民\"\n",
      "write file for \"[正妹] 瘦身日本妹\"\n"
     ]
    }
   ],
   "source": [
    "# start soup\n",
    "# root_soup is current soup on root\n",
    "start_soup = find_soup\n",
    "## print(start_soup.find_all_previous('div', 'r-ent'))\n",
    "## print(type(start_soup.find_all_previous('div', 'r-ent')))\n",
    "# open file\n",
    "all_file = codecs.open(\"all_articles.txt\", \"w\", \"utf-8\")\n",
    "pop_file = codecs.open(\"all_popular.txt\", \"w\", \"utf-8\")\n",
    "## f.write(\"Now the file has one more line!\")\n",
    "\n",
    "\n",
    "# first page is special case \n",
    "write_info(all_file, pop_file, start_soup)\n",
    "for cur_soup in start_soup.find_all_previous('div', 'r-ent'):\n",
    "    if get_title(cur_soup):\n",
    "        print('write file for \"'+get_title(cur_soup)+'\"')\n",
    "    write_info(all_file, pop_file, cur_soup)\n",
    "\n",
    "# second page start\n",
    "is_continued = True\n",
    "while is_continued:\n",
    "    # to next page\n",
    "    url = get_prev_page_url(root_soup)\n",
    "    response = get_response(url)\n",
    "    root_soup = BeautifulSoup(response.text, 'lxml')\n",
    "    \n",
    "    # loop root_soup\n",
    "    for cur_soup in reversed(root_soup.find_all('div', 'r-ent')):\n",
    "        title = get_title(cur_soup)\n",
    "        if title:\n",
    "            print('write file for \"'+title+'\"')\n",
    "        write_info(all_file, pop_file, cur_soup)\n",
    "        # ending point\n",
    "        global END_POST\n",
    "        if title == END_POST:\n",
    "            is_continued = False\n",
    "            break\n",
    "    is_continued == False\n",
    "all_file.close()\n",
    "pop_file.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<div class=\"title\">\n",
      "    <a href=\"/bbs/Beauty/M.1504367479.A.63D.html\">[正妹] 乃木坂<span class=\"__cf_email__\" data-cfemail=\"093d3f495d4e4a\">[email&#160;protected]</span></a>\n",
      "</div>\n",
      "\n",
      "[正妹] 乃木坂\n",
      "[email protected]\n"
     ]
    }
   ],
   "source": [
    "text =\\\n",
    "\"\"\"\n",
    "<div class=\"title\">\n",
    "    <a href=\"/bbs/Beauty/M.1504367479.A.63D.html\">[正妹] 乃木坂<span class=\"__cf_email__\" data-cfemail=\"093d3f495d4e4a\">[email&#160;protected]</span></a>\n",
    "</div>\n",
    "\"\"\"\n",
    "print(text)\n",
    "test = BeautifulSoup(text, 'lxml')\n",
    "for i in test.a.strings:\n",
    "    print(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
